{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d723b41f-79b4-4cb4-9296-fa2a20bbcf05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %pip install --upgrade databricks-sdk==0.49.0\n",
    "# %restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65cd643c-1192-4c46-8d6b-52de5c594154",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "get params"
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.dropdown(\"env\",\"sandbox\",[\"sandbox\",\"prod\"])\n",
    "dbutils.widgets.text(\"config_file_path\",\"\")\n",
    "dbutils.widgets.dropdown(\"job_type\",\"setup_framework\",[\"setup_framework\",\"data_sync\",\"incremental_ingestion\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e284e49c-a12a-417e-8f70-da690c6d1846",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "imports"
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk.service.jobs import JobSettings as Job\n",
    "from databricks.sdk.service import jobs as jobs_svc\n",
    "from databricks.sdk.service.jobs import JobAccessControlRequest, JobPermissionLevel\n",
    "from databricks.sdk.service import iam\n",
    "from typing import Dict, Any\n",
    "from databricks.sdk import WorkspaceClient\n",
    "import yaml\n",
    "from pprint import pprint\n",
    "from copy import deepcopy\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b7587fb-8b1f-4592-8012-07e027fddb3c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "init vars"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cluster_tags': {'resource_Type': 'etl-jobs', 'sub_team': 'Home'},\n",
      " 'driver_instance_pool_id': 'xxxx',\n",
      " 'instance_pool_id': 'xxxx',\n",
      " 'permissions': {'can_manage': 'admins',\n",
      "                 'can_manage_run': 'foo@bar.com, user@org.com',\n",
      "                 'can_view': 'view_user@org.com'},\n",
      " 'policy_id': 'xxxx',\n",
      " 'spn_id': 'xxxx',\n",
      " 'timeout_seconds': 7200,\n",
      " 'warning_threshold_seconds': 3600}\n",
      "[{'email_notification_list': 'sample_email@org.com',\n",
      "  'job_name': 'jb_incr_ing_<group_name_from_config>',\n",
      "  'job_parameters': {'base_checkpoint_path': 'abfss://container@sa.dfs.core.windows.net/ingestion_checkpoints',\n",
      "                     'group_name': '<group_name_from_config>'},\n",
      "  'notebook_path': '<notebook_path_in_workspace>',\n",
      "  'schedule': '10 30 8 * * ?',\n",
      "  'spark_conf': None},\n",
      " {'email_notification_list': 'sample_email@org.com, sample2@org.com',\n",
      "  'is_queue_enabled': False,\n",
      "  'job_name': 'jb_incr_ing_<group_name_from_config>',\n",
      "  'job_parameters': None,\n",
      "  'max_concurrent_runs': 1,\n",
      "  'notebook_path': '/Workspace/Shared/path/to/notebook',\n",
      "  'pause_schedule': False,\n",
      "  'schedule': '10 30 8 * * ?',\n",
      "  'spark_conf': None,\n",
      "  'tags': {'developer_name': 'user', 'job_type': 'incremental_ingestion'},\n",
      "  'timeout_seconds': 21600,\n",
      "  'warning_threshold_seconds': 7200}]\n"
     ]
    }
   ],
   "source": [
    "env = dbutils.widgets.get(\"env\")\n",
    "job_type = dbutils.widgets.get(\"job_type\")\n",
    "config_file_path = dbutils.widgets.get(\"config_file_path\")\n",
    "config_file_path = config_file_path.strip()\n",
    "if config_file_path == '':\n",
    "    if job_type == \"setup_framework\":\n",
    "        config_file_path = \"./setup_framework_jobs.yml\"\n",
    "    elif job_type == \"data_sync\":\n",
    "        config_file_path = \"./data_sync_jobs.yml\"\n",
    "    else:\n",
    "        config_file_path = './validation_job_config.yml'\n",
    "\n",
    "if not Path(config_file_path).exists() or not Path(config_file_path).is_file():\n",
    "  raise Exception(f\"Config file not found at {config_file_path}\")\n",
    "\n",
    "d_config = None\n",
    "with open(config_file_path) as stream:\n",
    "    try:\n",
    "        d_config = yaml.safe_load(stream)\n",
    "    except yaml.YAMLError as exc:\n",
    "        raise\n",
    "\n",
    "env_config = d_config.get(env)\n",
    "pprint(env_config)\n",
    "\n",
    "jobs = d_config.get('jobs')\n",
    "pprint(jobs)\n",
    "\n",
    "w = WorkspaceClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b49fcc4-3b98-46dc-8b0c-02e1edd310fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_job_owner(job_id):\n",
    "  response = w.jobs.get_permissions(job_id=job_id)\n",
    "  l_permissions = response.access_control_list\n",
    "  d_res = {}\n",
    "  owner = None\n",
    "  owner_type = None\n",
    "  for acl in l_permissions:\n",
    "    identity = acl.user_name or acl.group_name or acl.service_principal_name\n",
    "    id_type = 'user' if acl.user_name else 'group' if acl.group_name else 'service_principal'\n",
    "    for _perm in acl.all_permissions:\n",
    "      if _perm.permission_level == JobPermissionLevel.IS_OWNER:\n",
    "        owner = identity\n",
    "        owner_type = id_type\n",
    "        break\n",
    "  return owner, owner_type\n",
    "  \n",
    "def update_permissions(job_id, d_permissions:Dict[str,str]):\n",
    "  l_acls = []\n",
    "  owner, owner_type = get_job_owner(job_id)\n",
    "  owner_key = f\"{owner_type}:{owner}\"\n",
    "  for access_level, members in d_permissions.items():\n",
    "    members = [x.strip() for x in members.split(',')] if members else []\n",
    "    for x in members:\n",
    "      id_type = 'group'\n",
    "      identity = x\n",
    "      _l_x = [y.strip() for y in x.split(':') if y.strip() != '']\n",
    "      if len(_l_x) > 1:\n",
    "        id_type = str(_l_x[0]).strip().lower()\n",
    "        identity = _l_x[1]\n",
    "      key = f\"{id_type}:{identity}\"\n",
    "      if key == owner_key:\n",
    "        print(f\"Cannot modify owner pivileges for {owner_type} {owner}. If owner needs to be changed, contact admin.\")\n",
    "        continue\n",
    "      user_name = identity if id_type == 'user' else None\n",
    "      group_name = identity if id_type == 'group' else None\n",
    "      service_principal_name = identity if id_type in ('service_principal','spn') else None\n",
    "      required_access_level = (\n",
    "        JobPermissionLevel.CAN_MANAGE if access_level == 'can_manage' \n",
    "        else JobPermissionLevel.CAN_MANAGE_RUN if access_level == 'can_manage_run' \n",
    "        else JobPermissionLevel.CAN_VIEW\n",
    "      )\n",
    "      acl = JobAccessControlRequest(user_name = user_name, group_name = group_name, service_principal_name=service_principal_name, permission_level=required_access_level)\n",
    "      l_acls.append(acl)\n",
    "    # end of for\n",
    "  # end of for\n",
    "  if l_acls:\n",
    "    w.jobs.update_permissions(job_id=job_id, access_control_list=l_acls)\n",
    "    print(f\"Permissions updated for job {job_id}\")\n",
    "  \n",
    "\n",
    "\n",
    "      \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0671bc7-7881-429a-93b7-2996e6e8a346",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "templates"
    }
   },
   "outputs": [],
   "source": [
    "cluster_template = \"\"\"\n",
    "{\n",
    "  \"job_cluster_key\": \"{cluster_name}\",\n",
    "  \"new_cluster\": {\n",
    "      \"cluster_name\": \"\",\n",
    "      \"spark_version\": \"16.4.x-scala2.12\",\n",
    "      \"spark_env_vars\": {\n",
    "          \"PYSPARK_PYTHON\": \"/databricks/python3/bin/python3\",\n",
    "      },\n",
    "      \"instance_pool_id\": \"{instance_pool_id}\",\n",
    "      \"policy_id\": \"{policy_id}\",\n",
    "      \"driver_instance_pool_id\": \"{driver_instance_pool_id}\",\n",
    "      \"data_security_mode\": \"USER_ISOLATION\",\n",
    "      \"runtime_engine\": \"STANDARD\",\n",
    "      \"kind\": \"CLASSIC_PREVIEW\",\n",
    "      \"is_single_node\": False,\n",
    "      \"autoscale\": {\n",
    "          \"min_workers\": {min_workers},\n",
    "          \"max_workers\": {max_workers},\n",
    "      },\n",
    "  },\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "task_template = \"\"\"\n",
    "{\n",
    "  \"task_key\": \"{job_name}\",\n",
    "  \"notebook_task\": {\n",
    "      \"notebook_path\": \"{notebook_path}\",\n",
    "      \"source\": \"WORKSPACE\",\n",
    "  },\n",
    "  \"job_cluster_key\": \"{cluster_name}\",\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c875599d-27b8-48e2-827e-e4265bdb83a5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "helper functions"
    }
   },
   "outputs": [],
   "source": [
    "def get_cluster(cluster_name:str, \n",
    "                instance_pool_id:str, \n",
    "                policy_id:str, \n",
    "                driver_instance_pool_id:str, \n",
    "                min_workers:int, max_workers:int,\n",
    "                custom_tags:Dict[str,str] = None,\n",
    "                spark_conf:Dict[str,str] = None\n",
    "                ):\n",
    "    cluster = cluster_template.replace(\"{cluster_name}\",cluster_name)\n",
    "    cluster = cluster.replace(\"{instance_pool_id}\",instance_pool_id)\n",
    "    cluster = cluster.replace(\"{policy_id}\",policy_id)\n",
    "    cluster = cluster.replace(\"{driver_instance_pool_id}\",driver_instance_pool_id)\n",
    "    cluster = cluster.replace(\"{min_workers}\",str(min_workers))\n",
    "    cluster = cluster.replace(\"{max_workers}\",str(max_workers))\n",
    "    d_cluster =  eval(cluster)\n",
    "    if custom_tags:\n",
    "      d_cluster[\"new_cluster\"][\"custom_tags\"] = custom_tags\n",
    "    if spark_conf:\n",
    "        d_cluster[\"new_cluster\"][\"spark_conf\"] = spark_conf\n",
    "    return d_cluster\n",
    "\n",
    "def get_task(job_name:str,\n",
    "             cluster_name:str, \n",
    "             notebook_path:str, \n",
    "             job_parameters:Dict[str,Any]\n",
    "             ):\n",
    "    task = task_template.replace(\"{job_name}\",job_name)\n",
    "    task = task.replace(\"{notebook_path}\",notebook_path)\n",
    "    task = task.replace(\"{cluster_name}\", cluster_name)\n",
    "    # task = task.replace(\"{job_parameters}\",str(job_parameters))\n",
    "    d_task =  eval(task)\n",
    "    if job_parameters:\n",
    "      d_task[\"notebook_task\"][\"base_parameters\"] = job_parameters\n",
    "    return d_task\n",
    "\n",
    "def get_job_parameters(job_parameters:Dict[str,str]):\n",
    "  if not job_parameters:\n",
    "    return []\n",
    "  l_params = []\n",
    "  for k,v in job_parameters.items():\n",
    "    job_param = {\n",
    "      \"name\":k,\n",
    "      \"default\":v\n",
    "    }\n",
    "    l_params.append(job_param)\n",
    "  # end of for\n",
    "  return l_params\n",
    "\n",
    "def get_schedule(cron_expression:str, paused_status:str=\"PAUSED\"):\n",
    "    return {\n",
    "            \"quartz_cron_expression\": cron_expression,\n",
    "            \"timezone_id\": \"Asia/Kolkata\",\n",
    "            \"pause_status\": paused_status,\n",
    "        }\n",
    "\n",
    "def check_if_job_exists(job_name:str):\n",
    "  try:\n",
    "    jobs = [j for j in w.jobs.list(name=job_name)]\n",
    "    if not jobs:\n",
    "      return None\n",
    "    if len(jobs) > 1:\n",
    "        ids = \", \".join(str(j.job_id) for j in jobs)\n",
    "        raise ValueError(f\"Multiple jobs found with name '{job_name}': {ids}\")\n",
    "    # end of if\n",
    "    return jobs[0].job_id\n",
    "  except Exception as e:\n",
    "    print(\"Error encountered while checking if job exists\")\n",
    "    print(e)\n",
    "    return None\n",
    "\n",
    "def create_job(job_name:str, job_attribs:Dict[str,Any]):\n",
    "  # try:\n",
    "  jb = Job.from_dict(job_attribs)\n",
    "  job_id = check_if_job_exists(job_name)\n",
    "  if job_id:\n",
    "    print(f\"Job already exists with job_id:{job_id}, updating..\")\n",
    "    w.jobs.reset(new_settings=jb, job_id=job_id)\n",
    "  else:\n",
    "    print(f\"Creating new job {job_name}\")\n",
    "    jb = w.jobs.create(**jb.as_shallow_dict())\n",
    "    job_id = jb.job_id\n",
    "  return job_id\n",
    "  # except Exception as e:\n",
    "  #   print(f\"Error encountered while creating job: {job_name}\")\n",
    "  #   print(e)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d465a3e6-95db-47f1-b126-8c651b6ca2a2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "create job from config"
    }
   },
   "outputs": [],
   "source": [
    "def create_job_from_config(job_name,job_config):\n",
    "  notebook_path = elem_config.get('notebook_path')\n",
    "  job_parameters = elem_config.get('job_parameters')\n",
    "  driver_instance_pool_id = elem_config.get('driver_instance_pool_id')\n",
    "  instance_pool_id = elem_config.get('instance_pool_id')\n",
    "  policy_id = elem_config.get('policy_id')\n",
    "  min_workers = elem_config.get('min_workers',2)\n",
    "  max_workers = elem_config.get('max_workers',5)\n",
    "  spn_id = elem_config.get('spn_id')\n",
    "  cluster_name = f\"{job_name[:10]}_cluster\"\n",
    "  cluster_tags = elem_config.get('cluster_tags',{})\n",
    "  spark_conf = elem_config.get('spark_conf',{})\n",
    "  if not isinstance(cluster_tags,dict):\n",
    "    print(\"warning: cluster_tags cant be applied since not in `Dict[str,str] format, kindly modify the yml file.\")\n",
    "    cluster_tags = {}\n",
    "  d_cluster = get_cluster(cluster_name, instance_pool_id, policy_id, driver_instance_pool_id, min_workers, max_workers, cluster_tags, spark_conf)\n",
    "  d_task = get_task(job_name,cluster_name, notebook_path, job_parameters)\n",
    "  l_job_params = get_job_parameters(job_parameters)\n",
    "  email_notify = elem_config.get('email_notification_list',\"\")\n",
    "  l_email_notify = [x.strip() for x in email_notify.split(\",\")] if email_notify else []\n",
    "  _schedule = elem_config.get('schedule')\n",
    "  if _schedule and _schedule.strip()!='':\n",
    "    pause_schedule = elem_config.get('pause_schedule',False)\n",
    "    paused_status = 'PAUSED' if pause_schedule else 'UNPAUSED'\n",
    "    schedule = get_schedule(_schedule, paused_status=paused_status)\n",
    "  else:\n",
    "    schedule = None\n",
    "  timeout_seconds = elem_config.get('timeout_seconds',7200)\n",
    "  warning_threshold_seconds = elem_config.get('warning_threshold_seconds',3600)\n",
    "  max_concurrent_runs = elem_config.get('max_concurrent_runs',1)\n",
    "  queue_enabled = elem_config.get('is_queue_enabled',True)\n",
    "  \n",
    "  tags = elem_config.get('tags',{})\n",
    "  if not isinstance(tags,dict):\n",
    "    print(\"warning: tags cant be applied since not in `Dict[str,str] format, kindly modify the yml file.\")\n",
    "    tags = {}\n",
    "  # pprint(d_cluster)\n",
    "  # pprint(d_task)\n",
    "  d_job = {\n",
    "    \"name\": job_name,\n",
    "    \"tasks\":[d_task],\n",
    "    \"job_clusters\":[d_cluster],\n",
    "    \"queue\":{\n",
    "      \"enabled\": True\n",
    "    }\n",
    "  }\n",
    "  if spn_id and str(spn_id).strip()!='':\n",
    "    d_job[\"run_as\"] =  {\n",
    "      \"service_principal_name\": spn_id\n",
    "    }\n",
    "  if l_job_params:\n",
    "    d_job[\"parameters\"]=l_job_params\n",
    "  if l_email_notify:\n",
    "    d_job[\"email_notifications\"]={\n",
    "      \"on_failure\": l_email_notify,\n",
    "      \"no_alert_for_skipped_runs\": True,\n",
    "    }\n",
    "  if schedule:\n",
    "    d_job[\"schedule\"] = schedule\n",
    "\n",
    "  if timeout_seconds:\n",
    "    d_job[\"timeout_seconds\"]=timeout_seconds\n",
    "\n",
    "  if warning_threshold_seconds:\n",
    "    d_job[\"health\"] = {\"rules\": []}\n",
    "    d_job[\"health\"][\"rules\"].append({\n",
    "            \"metric\": \"RUN_DURATION_SECONDS\",\n",
    "            \"op\": \"GREATER_THAN\",\n",
    "            \"value\": warning_threshold_seconds\n",
    "        })\n",
    "    d_job[\"email_notifications\"][\"on_duration_warning_threshold_exceeded\"]=l_email_notify\n",
    "  if max_concurrent_runs:\n",
    "    d_job[\"max_concurrent_runs\"]=max_concurrent_runs\n",
    "  if tags:\n",
    "    d_job[\"tags\"] = tags\n",
    "  d_job[\"queue\"][\"enabled\"]=queue_enabled\n",
    "  # pprint(d_job)\n",
    "  job_id = create_job(job_name, d_job)\n",
    "  # print(f\"Job id: {job_id}\")\n",
    "  d_permissions = elem_config.get('permissions')\n",
    "  if d_permissions:\n",
    "    try:\n",
    "      update_permissions(job_id, d_permissions)\n",
    "    except Exception as e:\n",
    "      print(f\"Error encountered while updating permissions for job_name: {job_name}, permissions:{d_permissions}\")\n",
    "      print(e)\n",
    "      \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30d5d623-ccea-4b57-bab3-a325096968a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job already exists with job_id:855441139647785, updating..\n",
      "Cannot modify owner pivileges for user tanveer.singh@databricks.com. If owner needs to be changed, contact admin.\n",
      "Permissions updated for job 855441139647785\n",
      "Job already exists with job_id:756216877515660, updating..\n",
      "Permissions updated for job 756216877515660\n"
     ]
    }
   ],
   "source": [
    "for job_config in jobs:\n",
    "  elem_config = deepcopy(env_config)\n",
    "  elem_config.update(job_config)\n",
    "  job_name = elem_config.get('job_name')\n",
    "  create_job_from_config(job_name,elem_config)\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "job_deployment",
   "widgets": {
    "config_file_path": {
     "currentValue": "",
     "nuid": "ad308679-03f0-4386-8226-24dfc9c0d43b",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "config_file_path",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "",
      "label": null,
      "name": "config_file_path",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "env": {
     "currentValue": "sandbox",
     "nuid": "b427e27d-199f-46fb-bf96-75cb8e381edc",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "sandbox",
      "label": null,
      "name": "env",
      "options": {
       "choices": [
        "sandbox",
        "prod"
       ],
       "fixedDomain": true,
       "multiselect": false,
       "widgetDisplayType": "Dropdown"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "sandbox",
      "label": null,
      "name": "env",
      "options": {
       "autoCreated": null,
       "choices": [
        "sandbox",
        "prod"
       ],
       "widgetType": "dropdown"
      },
      "widgetType": "dropdown"
     }
    },
    "job_type": {
     "currentValue": "incremental_ingestion",
     "nuid": "abb2a4b6-cc8f-44ce-8a64-aa83ce6f427c",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "setup_framework",
      "label": null,
      "name": "job_type",
      "options": {
       "choices": [
        "setup_framework",
        "data_sync",
        "incremental_ingestion"
       ],
       "fixedDomain": true,
       "multiselect": false,
       "widgetDisplayType": "Dropdown"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "setup_framework",
      "label": null,
      "name": "job_type",
      "options": {
       "autoCreated": null,
       "choices": [
        "setup_framework",
        "data_sync",
        "incremental_ingestion"
       ],
       "widgetType": "dropdown"
      },
      "widgetType": "dropdown"
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
