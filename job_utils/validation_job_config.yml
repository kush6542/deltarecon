# ==============================================================================
# Global Job Settings
# ==============================================================================

# REQUIRED: Cluster Policy ID
# policy_id: ''

# Compute Configuration

# Option 1: Comment out instance pools 
# driver_instance_pool_id: '1102-051010-guppy41-pool-b9my9uhr'
# instance_pool_id: '1102-051010-guppy41-pool-b9my9uhr'

# Option 2: Use node types (for Azure - Standard_DS3_v2 is common)
# UNCOMMENT these and update with your preferred Azure VM size:
node_type_id: 'Standard_D8s_v3'
driver_node_type_id: 'Standard_D8s_v3'
# -----------------------------------------------------------------------------

spn_id: ''
min_workers: 2
max_workers: 8
spark_conf:
  spark.sql.adaptive.enabled: "true"
  spark.driver.extraJavaOptions: "-Duser.timezone=Asia/Kolkata"
  spark.executor.extraJavaOptions: "-Duser.timezone=Asia/Kolkata"
email_notification_list: utkarsh3.kumar@ril.com
cluster_tags:
  managed_by: validation_framework
  purpose: data_validation
job_tags:
  managed_by: validation_framework
timeout_seconds: 7200
warning_threshold_seconds: 3600
permissions:
  can_manage: group:admins
  can_manage_run: user:fname.lname@company.com
  can_view: group:analysts
max_concurrent_runs: 1
is_queue_enabled: false
pause_schedule: false
tags:
  managed_by: validation_framework
  table_group: comprehensive_test_group #group name from config/table_groups.yml
  environment: test # test or prod


# Individual job configs 
jobs:
  - job_name: validation_job_<table_group_name>
    notebook_path: '/Workspace/Users/user.name@company.com/deltarecon/notebooks/main' # Replace with path to main.py
    schedule: null
    job_parameters:
      table_group: <table_group_name>
      iteration_suffix: <suffix_name> # e.g. daily, hourly, etc.
      isFullValidation: "false" # True does hash based reconciliation, expensive !
    

